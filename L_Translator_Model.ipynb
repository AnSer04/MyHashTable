{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1WUhYxgKwZH8BsjpzfEWv2t7Tr3LjckHy",
      "authorship_tag": "ABX9TyO30arIiHMNOzuiMmOsVCvE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnSer04/MyHashTable/blob/main/L_Translator_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Подготовка текстовых данных для обучения"
      ],
      "metadata": {
        "id": "2IosluXjx8C5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Парсинг исходного файла словаря обучающей выборки и формирование sources и targets**\n",
        "\n",
        "1. Открытие архива словаря WordTheme на диске\n",
        "2. Чтение файла json из архива\n",
        "3. Разбор файла и выделение обучающих примеров\n",
        "4. Отделение исходных и целевых текстов в отдельные списки для дальнейшей работы с ними"
      ],
      "metadata": {
        "id": "RKllP-5gC94S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile as ziplib\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Пути к файлам\n",
        "word_theme_zip_file_path = \"/content/drive/MyDrive/WordTheme/Корпус переводов на лиасфирский_20240430_1431.wt\"\n",
        "dictionary_file = \"dictionary.txt\"\n",
        "\n",
        "# Чтение содержимого из zip файла\n",
        "with ziplib.ZipFile(word_theme_zip_file_path, \"r\") as zip_file:\n",
        "  with zip_file.open(dictionary_file) as file:\n",
        "    file_content = file.read().decode(\"utf-8\")\n",
        "\n",
        "# Выделение обучающих примеров в один список\n",
        "train_samples = [(item[\"m\"], item[\"t\"]) for item in json.loads(file_content)[\"lword\"]]\n",
        "\n",
        "# Формирование исходных и целевых списков для обучения\n",
        "sources = [sample[0] for sample in train_samples]\n",
        "targets = [sample[1] for sample in train_samples]\n",
        "\n",
        "# Визуализация данных для теста\n",
        "pd.DataFrame(\n",
        "    train_samples,\n",
        "    columns=[\"Русский\", \"Лиасфирский\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "mFdNDdEOmdde",
        "outputId": "d1051a2c-3fc2-4853-edea-84abb698409c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Русский  \\\n",
              "0     Здравствуйте! Очень рад, что смог вас застать ...   \n",
              "1     Здравствуйте! Очень рада, что смогла вас заста...   \n",
              "2     Всем привет! Я тут новый, поэтому хочу со всем...   \n",
              "3     Привет, ребята! Как вас зовут? Я тут совсем но...   \n",
              "4                           Здравствуйте, как ваше имя?   \n",
              "...                                                 ...   \n",
              "2529  Они мне надоели, поэтому я решил уйти, а они п...   \n",
              "2530  Они прожили вместе долгую и прекрасную жизнь, ...   \n",
              "2531  Я чувствоваю такое же, что и ты чувствовала то...   \n",
              "2532  Жил-был мальчик по имени Амир. Он жил в малень...   \n",
              "2533  Однажды Амиру приснился сон. В этом сне он уви...   \n",
              "\n",
              "                                            Лиасфирский  \n",
              "0     Al ū'sarezi! Af taÿ ki dorivaÿ az, st'uaksoėo ...  \n",
              "1     Al ū'sarezi! Af taï ki dorivaï, st'uaksao oaėt...  \n",
              "2     Ū'sarezi vöė-ū's! Af taÿ ki fenÿ ügup, aqaėo ĺ...  \n",
              "3     Ū'sarezi, paksië! Af önze uolaf akli? Af taï k...  \n",
              "4                        Ū'sarezi, af al öni uola akli?  \n",
              "...                                                 ...  \n",
              "2529  Hateėo voz vöė-voz, aupazeh veĺ, ūatea taÿ oėo...  \n",
              "2530  Inuaa voz le-nua ki beoai a soni ū'i, afizai v...  \n",
              "2531  Aėşeėo taÿ ho oks, a aėşea taoï ola ve-oaĺ. — ...  \n",
              "2532  Afiza rolavo apeėli lö li-uola Amir. Nuaa voa ...  \n",
              "2533  Ünaa Amir otau le-siovur. Efaėsia voa öz vo-e ...  \n",
              "\n",
              "[2534 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-82d372b1-9520-4323-ab20-cc8e0bf604a9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Русский</th>\n",
              "      <th>Лиасфирский</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Здравствуйте! Очень рад, что смог вас застать ...</td>\n",
              "      <td>Al ū'sarezi! Af taÿ ki dorivaÿ az, st'uaksoėo ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Здравствуйте! Очень рада, что смогла вас заста...</td>\n",
              "      <td>Al ū'sarezi! Af taï ki dorivaï, st'uaksao oaėt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Всем привет! Я тут новый, поэтому хочу со всем...</td>\n",
              "      <td>Ū'sarezi vöė-ū's! Af taÿ ki fenÿ ügup, aqaėo ĺ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Привет, ребята! Как вас зовут? Я тут совсем но...</td>\n",
              "      <td>Ū'sarezi, paksië! Af önze uolaf akli? Af taï k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Здравствуйте, как ваше имя?</td>\n",
              "      <td>Ū'sarezi, af al öni uola akli?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2529</th>\n",
              "      <td>Они мне надоели, поэтому я решил уйти, а они п...</td>\n",
              "      <td>Hateėo voz vöė-voz, aupazeh veĺ, ūatea taÿ oėo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2530</th>\n",
              "      <td>Они прожили вместе долгую и прекрасную жизнь, ...</td>\n",
              "      <td>Inuaa voz le-nua ki beoai a soni ū'i, afizai v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2531</th>\n",
              "      <td>Я чувствоваю такое же, что и ты чувствовала то...</td>\n",
              "      <td>Aėşeėo taÿ ho oks, a aėşea taoï ola ve-oaĺ. — ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2532</th>\n",
              "      <td>Жил-был мальчик по имени Амир. Он жил в малень...</td>\n",
              "      <td>Afiza rolavo apeėli lö li-uola Amir. Nuaa voa ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2533</th>\n",
              "      <td>Однажды Амиру приснился сон. В этом сне он уви...</td>\n",
              "      <td>Ünaa Amir otau le-siovur. Efaėsia voa öz vo-e ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2534 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-82d372b1-9520-4323-ab20-cc8e0bf604a9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-82d372b1-9520-4323-ab20-cc8e0bf604a9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-82d372b1-9520-4323-ab20-cc8e0bf604a9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b2cfce47-bc89-4424-8565-bcf14a3d5a92\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b2cfce47-bc89-4424-8565-bcf14a3d5a92')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b2cfce47-bc89-4424-8565-bcf14a3d5a92 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \")\",\n  \"rows\": 2534,\n  \"fields\": [\n    {\n      \"column\": \"\\u0420\\u0443\\u0441\\u0441\\u043a\\u0438\\u0439\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2516,\n        \"samples\": [\n          \"\\u041a\\u0430\\u043a\\u043e\\u0439 \\u043c\\u0430\\u043b\\u044c\\u0447\\u0438\\u043a \\u044d\\u0442\\u043e \\u0441\\u0434\\u0435\\u043b\\u0430\\u043b?\",\n          \"\\u0422\\u044b \\u0434\\u043e\\u043b\\u0436\\u0435\\u043d \\u0432\\u0441\\u043f\\u043e\\u043c\\u043d\\u0438\\u0442\\u044c \\u044d\\u0442\\u0443 \\u043f\\u0435\\u0441\\u043d\\u044e.\",\n          \"\\u0418\\u0434\\u044f \\u0432\\u0435\\u0447\\u0435\\u0440\\u043e\\u043c \\u0434\\u043e\\u043c\\u043e\\u0439 \\u043f\\u043e \\u043c\\u043e\\u0441\\u0442\\u0443, \\u0441\\u043c\\u043e\\u0442\\u0440\\u044f \\u043d\\u0430 \\u043d\\u0435\\u0431\\u043e, \\u044f \\u0432\\u0438\\u0434\\u0435\\u043b \\u043a\\u0440\\u0430\\u0441\\u0438\\u0432\\u043e \\u0438\\u0441\\u043a\\u0440\\u044f\\u0449\\u0438\\u0435\\u0441\\u044f \\u0437\\u0432\\u0451\\u0437\\u0434\\u044b.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u041b\\u0438\\u0430\\u0441\\u0444\\u0438\\u0440\\u0441\\u043a\\u0438\\u0439\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2522,\n        \"samples\": [\n          \"Sali, \\u00cfhave\",\n          \"Na\\u0117fa\\u0117i voa, af v\\u00f6\\u0117-voa ve-oa\\u013a, st'entah \\u016b'sone-le-vea\\u013a ni la\\u0117-fanz ez lo-goa.\",\n          \"So\\u0117ce li-st'i\\u00e9nt\\u00eff as afiza ta\\u00ff ez lo-do ki st'\\u016bai, ae as ua\\u0117tea goa\\u013avo, st'uvah ve-ta\\u00ff.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Настройка и обучение токенизаторов source_tokenizer и target_tokenizer**\n",
        "\n",
        "1. Инициализация токенизаторов\n",
        "2. Настройка для каждого токенизатора:\n",
        "    - предтокенизатора\n",
        "    - нормализатора токенов\n",
        "    - параметров постобработки последовательностей\n",
        "    - параметров обучение токенизаторов\n",
        "    - декодера для преобразования в текст на целевом языке\n",
        "3. Обучение токенизаторов\n",
        "4. Паддинг данных"
      ],
      "metadata": {
        "id": "fson7N-XKR8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tokenizers import Tokenizer, Regex\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "from tokenizers import normalizers\n",
        "from tokenizers.pre_tokenizers import Split\n",
        "from tokenizers.normalizers import Lowercase, Replace\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from tokenizers import decoders\n",
        "\n",
        "\n",
        "# Инициализация токенизаторов\n",
        "source_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\", dropout=0.1))\n",
        "target_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\", dropout=0.1))\n",
        "\n",
        "\n",
        "# Настройка исходного токенизатора\n",
        "split_pattern = Regex(r\"\\d|[\\w']+|[.,!?;:—\\-]\")\n",
        "\n",
        "source_tokenizer.pre_tokenizer = Split(pattern=split_pattern, behavior=\"removed\", invert=True)\n",
        "source_tokenizer.normalizer = normalizers.Sequence([\n",
        "    Lowercase(),\n",
        "    Replace(\"-\", \" \"),\n",
        "    Replace(\"—\", \"-\"),\n",
        "    Replace(\"ё\", \"е\")\n",
        "])\n",
        "\n",
        "source_tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[SOS] $A [EOS]\",\n",
        "    special_tokens=[\n",
        "        (\"[SOS]\", 2),\n",
        "        (\"[EOS]\", 3),\n",
        "    ],\n",
        ")\n",
        "\n",
        "source_trainer = WordPieceTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[SOS]\", \"[EOS]\"], vocab_size=2000, min_frequency=2)\n",
        "source_tokenizer.decoder = decoders.WordPiece()\n",
        "\n",
        "# Настройка целевого токенизатора\n",
        "split_pattern = Regex(r\"\\d|[\\w'\\-]+|[.,!?;:—]\")\n",
        "\n",
        "target_tokenizer.pre_tokenizer = Split(pattern=split_pattern, behavior=\"removed\", invert=True)\n",
        "target_tokenizer.normalizer = normalizers.Sequence([\n",
        "    Lowercase(),\n",
        "    Replace(\"—\", \"-\")\n",
        "])\n",
        "\n",
        "target_tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[SOS] $A [EOS]\",\n",
        "    special_tokens=[\n",
        "        (\"[SOS]\", 2),\n",
        "        (\"[EOS]\", 3),\n",
        "    ],\n",
        ")\n",
        "\n",
        "target_trainer = WordPieceTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[SOS]\", \"[EOS]\"], vocab_size=2000, min_frequency=2)\n",
        "target_tokenizer.decoder = decoders.WordPiece()\n",
        "\n",
        "# Обучение токенизаторов с нормализованными данными\n",
        "source_tokenizer.train_from_iterator(sources, trainer=source_trainer)\n",
        "target_tokenizer.train_from_iterator(targets, trainer=target_trainer)\n",
        "\n",
        "# Определение максимальной длины последовательности для паддинга\n",
        "source_max_length = max(len(source_tokenizer.encode(text).ids) for text in sources)\n",
        "target_max_length = max(len(source_tokenizer.encode(text).ids) for text in targets)\n",
        "max_length = max(source_max_length, target_max_length)\n",
        "\n",
        "# Паддинг\n",
        "source_tokenizer.enable_padding(max_length=max_length, pad_id=0, pad_token=\"[PAD]\")\n",
        "target_tokenizer.enable_padding(max_length=max_length, pad_id=0, pad_token=\"[PAD]\")"
      ],
      "metadata": {
        "id": "atsLb1ZAnU63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Небольшой тест токенизаторов**"
      ],
      "metadata": {
        "id": "pPF7N7Vd5ojp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Source Tokenizer: {source_tokenizer.get_vocab_size()} tokens\")\n",
        "print(f\"Target Tokenizer: {target_tokenizer.get_vocab_size()} tokens\")\n",
        "\n",
        "# Тест исходного токенизаторsа\n",
        "source_output = source_tokenizer.encode_batch([\n",
        "    \"Привет, я \\\"твой\\\" переводчик, который поможет тебе познакомиться с лиасфирским.\",\n",
        "    \"Я очень стараюсь научиться очень хорошо переводить на лиасфирский — прости если что.\",\n",
        "    \"Мечта, творчество - вот ключ к успеху, силе и чему-то новому!\",\n",
        "    \"Ёжик плыл по реке тихо-тихо и думал, где он сейчас.\",\n",
        "    \"У меня есть 123 яблока и 456 груш.\",\n",
        "    \"Почему так тяжело, просто переводить или перевести что-то на лиасфирский?\",\n",
        "    \"Я люблю тебя, а ты любишь ли меня?\",\n",
        "    \"Смог бы кто-нибудь помочь мне выбраться оттуда?\",\n",
        "    \"Собрать, выбрать, забрать, убрать, крыть, открыть, закрыть, укрыть, раскрыть, покрыть, покрытие\"\n",
        "\n",
        "])\n",
        "\n",
        "def pretty_tokenized_print(encoded_text, tokenizer):\n",
        "    print(f\"\\n[{len(encoded_text.tokens)} tokens in length]\")\n",
        "    print(f\"INPUT: {tokenizer.decode(encoded_text.ids)}\")\n",
        "    tokens = encoded_text.tokens\n",
        "    ids = [id for id in encoded_text.ids if id > 0]\n",
        "    if \"[PAD]\" in tokens:\n",
        "        index = tokens.index(\"[PAD]\")\n",
        "        tokens = tokens[:index] + [\"...\"]\n",
        "    print(\"TOKENS:\", *tokens)\n",
        "    print(\"IDs:\", ids)\n",
        "\n",
        "\n",
        "\n",
        "for encoded_text in source_output:\n",
        "    pretty_tokenized_print(encoded_text, source_tokenizer)\n",
        "\n",
        "print(\"-\"*40)\n",
        "\n",
        "# Тест целевого токенизатора\n",
        "target_output = target_tokenizer.encode_batch([\n",
        "    \"Ū'sarezi, aėni, af taÿ öni lauoĺin ki anaėsehoė vöė-taoÿ apūaėvah le-liasfir\",\n",
        "    \"Neėvaėo taÿ az, st'uaksavo lauoĺh laė-liasfir leĺ, se naėni ö vo-oaĺvo ki lėo\",\n",
        "    \"Faĺ, vellofas: af o tos ki avifosai ksoĺ li-anaėa, li-raĺ a ksoĺ vi-oaĺvo ki feni evaėvos!\",\n",
        "    \"Faävai voa assioli a ūavai, af voa üva st'aė\",\n",
        "    \"Ünaėo taÿ 123 le-habruz a 456 le-hrosïz.\",\n",
        "    \"Af e ki eraĺi ho uahaėo, lauoĺh laė-liasfir?\",\n",
        "    \"Ūaė taÿ ve-taoï, ae taoï ūaėö taÿk\",\n",
        "    \"St'uaksavo anaėseh goaĺvo vöė-taoï, evah eźźiú?\",\n",
        "    \"Af taÿ tefak naėcoh onoė ve-oaĺvo vu. ifo avifosaÿ\",\n",
        "    \"Aĺzeh, öuzeh, oaėzeh, faėzeh, ezeh, uazeh, uazen, laorozeh, tueėzeh, ueklizeh, hekzeh, aėzeh, uėzeh\",\n",
        "    \"Lele-ūab, lele-oaė, le-öu, le-oaė, tavo, tavoÿ, tavoï, tavoi, tavoin, tavoinz, tavoinaë, ū'sarezi, ū'sareziaë, ū'sareziz, ū'sarezih, ū'sareziao, ū'sarezihoė, ū'sarezihoz\"\n",
        "])\n",
        "\n",
        "for encoded_text in target_output:\n",
        "   pretty_tokenized_print(encoded_text, target_tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfSuM0I9KCdI",
        "outputId": "1b2810fa-4933-4df7-e32e-7066ed95e35c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Tokenizer: 2000 tokens\n",
            "Target Tokenizer: 2000 tokens\n",
            "\n",
            "[94 tokens in length]\n",
            "INPUT: привет, я твой переводчик, который поможет тебе познакомиться с лиасфирским.\n",
            "TOKENS: [SOS] привет , я твой перево ##д ##чи ##к , который поможет тебе познакомиться с лиасфирс ##ким . [EOS] ...\n",
            "IDs: [2, 369, 5, 50, 746, 1581, 67, 190, 72, 5, 327, 1200, 246, 755, 36, 1062, 1414, 7, 3]\n",
            "\n",
            "[94 tokens in length]\n",
            "INPUT: я очень стараюсь научиться очень хорошо переводить на лиасфирский - прости если что.\n",
            "TOKENS: [SOS] я очень ста ##ра ##юсь на ##учи ##ться очень хорошо перево ##дить на лиасфирс ##кий - прости если что . [EOS] ...\n",
            "IDs: [2, 50, 208, 469, 101, 596, 92, 1171, 242, 208, 399, 1581, 603, 92, 1062, 493, 6, 673, 285, 97, 7, 3]\n",
            "\n",
            "[94 tokens in length]\n",
            "INPUT: мечта, творчество вот ключ к успеху, силе и чему то новому!\n",
            "TOKENS: [SOS] мечт ##а , тво ##р ##че ##ство вот к ##лю ##ч к у ##с ##пе ##х ##у , сил ##е и чем ##у то ново ##му ! [EOS] ...\n",
            "IDs: [2, 1459, 58, 5, 238, 57, 340, 348, 521, 29, 241, 66, 29, 38, 54, 490, 77, 70, 5, 1243, 56, 27, 404, 70, 116, 703, 191, 4, 3]\n",
            "\n",
            "[94 tokens in length]\n",
            "INPUT: ежик плыл по реке тихо тихо и думал, где он сейчас.\n",
            "TOKENS: [SOS] е ##жи ##к п ##лы ##л по ре ##ке тихо тихо и думал , где он сейчас . [EOS] ...\n",
            "IDs: [2, 24, 321, 72, 34, 730, 62, 85, 311, 361, 1624, 1624, 27, 831, 5, 276, 164, 418, 7, 3]\n",
            "\n",
            "[94 tokens in length]\n",
            "INPUT: у меня есть 1 2 яблока и 4 5 6 груш.\n",
            "TOKENS: [SOS] у меня есть 1 2 [UNK] ябло ##ка и 4 5 6 гру ##ш . [EOS] ...\n",
            "IDs: [2, 38, 183, 280, 9, 10, 1, 888, 106, 27, 11, 12, 13, 889, 71, 7, 3]\n",
            "\n",
            "[94 tokens in length]\n",
            "INPUT: почему так тяжело, просто переводить или перевести что то на лиасфирский?\n",
            "TOKENS: [SOS] почему так тяж ##ело , просто перево ##дить или пере ##вест ##и что то на лиасфирс ##кий ? [EOS] ...\n",
            "IDs: [2, 342, 135, 1639, 414, 5, 473, 1581, 603, 401, 439, 1285, 65, 97, 116, 92, 1062, 493, 18, 3]\n",
            "\n",
            "[94 tokens in length]\n",
            "INPUT: я люблю тебя, а ты любишь ли меня?\n",
            "TOKENS: [SOS] я люблю тебя , а ты люби ##шь ли меня ? [EOS] ...\n",
            "IDs: [2, 50, 435, 207, 5, 19, 117, 1198, 145, 301, 183, 18, 3]\n",
            "\n",
            "[94 tokens in length]\n",
            "INPUT: смог бы кто нибудь помочь мне выбраться оттуда?\n",
            "TOKENS: [SOS] смог бы кто нибудь помочь мне вы ##б ##рать ##ся от ##ту ##да ? [EOS] ...\n",
            "IDs: [2, 1563, 119, 214, 839, 780, 185, 154, 80, 1265, 111, 141, 292, 98, 18, 3]\n",
            "\n",
            "[94 tokens in length]\n",
            "INPUT: собрать, выбрать, забрать, убрать, крыть, открыть, закрыть, укрыть, раскрыть, покрыть, покрытие\n",
            "TOKENS: [SOS] соб ##рать , вы ##б ##рать , заб ##рать , уб ##рать , кры ##ть , откры ##ть , закры ##ть , у ##кры ##ть , рас ##кры ##ть , по ##кры ##ть , по ##кры ##ти ##е [EOS] ...\n",
            "IDs: [2, 308, 1265, 5, 154, 80, 1265, 5, 1103, 1265, 5, 1358, 1265, 5, 1350, 84, 5, 1277, 84, 5, 1451, 84, 5, 38, 902, 84, 5, 372, 902, 84, 5, 85, 902, 84, 5, 85, 902, 147, 56, 3]\n",
            "----------------------------------------\n",
            "\n",
            "[94 tokens in length]\n",
            "INPUT: ū'sarezi, aėni, af taÿ öni lauoĺin ki anaėsehoė vöė-taoÿ apūaėvah le-liasfir\n",
            "TOKENS: [SOS] ū'sarezi , aėni , af taÿ öni lauoĺ ##in ki anaėseh ##oė vöė-taoÿ apūaė ##vah le-lia ##s ##fir [EOS] ...\n",
            "IDs: [2, 287, 6, 579, 6, 122, 137, 242, 1486, 525, 127, 639, 130, 360, 1765, 289, 1315, 77, 445, 3]\n",
            "\n",
            "[94 tokens in length]\n",
            "INPUT: neėvaėo taÿ az, st'uaksavo lauoĺh laė-liasfir leĺ, se naėni ö vo-oaĺvo ki lėo\n",
            "TOKENS: [SOS] ne ##ė ##vaėo taÿ az , st'uaksavo lauoĺ ##h laė-liasfir leĺ , se naėni ö vo- ##oaĺ ##vo ki l ##ėo [EOS] ...\n",
            "IDs: [2, 1189, 70, 911, 137, 254, 6, 861, 1486, 73, 1132, 402, 6, 189, 640, 53, 255, 143, 128, 127, 31, 199, 3]\n",
            "\n",
            "[94 tokens in length]\n",
            "INPUT: faĺ, vellofas : af o tos ki avifosai ksoĺ li-anaėa, li-raĺ a ksoĺ vi-oaĺvo ki feni evaėvos!\n",
            "TOKENS: [SOS] faĺ , vello ##fas : af o to ##s ki av ##ifo ##sai ksoĺ li-a ##naė ##a , li- ##raĺ a ksoĺ vi-oaĺvo ki feni evaėvos ! [EOS] ...\n",
            "IDs: [2, 1796, 6, 819, 467, 18, 122, 34, 174, 77, 127, 422, 220, 1223, 228, 631, 465, 69, 6, 148, 740, 20, 228, 1736, 127, 565, 1564, 4, 3]\n",
            "\n",
            "[94 tokens in length]\n",
            "INPUT: faävai voa assioli a ūavai, af voa üva st'aė\n",
            "TOKENS: [SOS] faä ##va ##i voa assi ##oli a ūava ##i , af voa üva st'aė [EOS] ...\n",
            "IDs: [2, 1941, 150, 81, 207, 1464, 914, 20, 431, 81, 6, 122, 207, 346, 283, 3]\n",
            "\n",
            "[94 tokens in length]\n",
            "INPUT: ünaėo taÿ 1 2 le-habruz a 4 5 6 le-hrosïz.\n",
            "TOKENS: [SOS] ünaėo taÿ 1 2 [UNK] le-habru ##z a 4 5 6 le- ##hros ##ïz . [EOS] ...\n",
            "IDs: [2, 391, 137, 10, 11, 1, 1763, 80, 20, 12, 13, 14, 124, 1293, 592, 8, 3]\n",
            "\n",
            "[94 tokens in length]\n",
            "INPUT: af e ki eraĺi ho uahaėo, lauoĺh laė-liasfir?\n",
            "TOKENS: [SOS] af e ki eraĺi ho uahaėo , lauoĺ ##h laė-liasfir ? [EOS] ...\n",
            "IDs: [2, 122, 24, 127, 1778, 200, 702, 6, 1486, 73, 1132, 19, 3]\n",
            "\n",
            "[94 tokens in length]\n",
            "INPUT: ūaė taÿ ve-taoï, ae taoï ūaėö taÿk\n",
            "TOKENS: [SOS] ūaė taÿ ve-taoï , ae taoï ūaė ##ö taÿ ##k [EOS] ...\n",
            "IDs: [2, 225, 137, 693, 6, 279, 230, 225, 72, 137, 92, 3]\n",
            "\n",
            "[94 tokens in length]\n",
            "INPUT: st'uaksavo anaėseh goaĺvo vöė-taoï, evah eźźiú?\n",
            "TOKENS: [SOS] st'uaksavo anaėseh goaĺvo vöė-taoï , eva ##h e ##ź ##źiú ? [EOS] ...\n",
            "IDs: [2, 861, 639, 510, 511, 6, 829, 73, 24, 113, 1072, 19, 3]\n",
            "\n",
            "[94 tokens in length]\n",
            "INPUT: af taÿ tefak naėcoh onoė ve-oaĺvo vu. ifo avifosaÿ\n",
            "TOKENS: [SOS] af taÿ tefak naėcoh onoė ve-oaĺvo vu . ifo av ##ifo ##sa ##ÿ [EOS] ...\n",
            "IDs: [2, 122, 137, 1178, 906, 1701, 306, 218, 8, 355, 422, 220, 133, 103, 3]\n",
            "\n",
            "[94 tokens in length]\n",
            "INPUT: aĺzeh, öuzeh, oaėzeh, faėzeh, ezeh, uazeh, uazen, laorozeh, tueėzeh, ueklizeh, hekzeh, aėzeh, uėzeh\n",
            "TOKENS: [SOS] aĺzeh , öu ##zeh , oaėzeh , faė ##zeh , ez ##eh , ua ##zeh , ua ##ze ##n , la ##oro ##zeh , tue ##ė ##zeh , ue ##kli ##zeh , he ##k ##zeh , aė ##zeh , uė ##zeh [EOS] ...\n",
            "IDs: [2, 470, 6, 384, 930, 6, 1965, 6, 211, 930, 6, 157, 340, 6, 210, 930, 6, 210, 175, 83, 6, 173, 1596, 930, 6, 1058, 70, 930, 6, 961, 1849, 930, 6, 1006, 92, 930, 6, 151, 930, 6, 1436, 930, 3]\n",
            "\n",
            "[94 tokens in length]\n",
            "INPUT: lele-ūab, lele-oaė, le-öu, le-oaė, tavo, tavoÿ, tavoï, tavoi, tavoin, tavoinz, tavoinaë, ū'sarezi, ū'sareziaë, ū'sareziz, ū'sarezih, ū'sareziao, ū'sarezihoė, ū'sarezihoz\n",
            "TOKENS: [SOS] lele- ##ūa ##b , lele- ##oaė , le-öu , le-oaė , tavo , tavo ##ÿ , tavo ##ï , tavoi , tavoi ##n , tavoi ##nz , tavoi ##naë , ū'sarezi , ū'sareziaë , ū'sarezi ##z , ū'sarezi ##h , ū'sarezi ##ao , ū'sarezi ##hoė , ū'sarezi ##hoz [EOS] ...\n",
            "IDs: [2, 339, 444, 102, 6, 339, 799, 6, 1622, 6, 1869, 6, 1026, 6, 1026, 103, 6, 1026, 84, 6, 555, 6, 555, 83, 6, 555, 551, 6, 555, 615, 6, 287, 6, 858, 6, 287, 80, 6, 287, 73, 6, 287, 264, 6, 287, 212, 6, 287, 1263, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Создание датасетов и загрузчиков данных train_data_loader и val_data_loader для модели**\n",
        "\n",
        "1. Создание класса датасета, наследуясь от датасета из торча\n",
        "2. Создание функции разделение на обучающую и валидационные выборки\n",
        "3. Создание датасета для входных данных\n",
        "4. Формирование загрузчиков данных"
      ],
      "metadata": {
        "id": "4sqE3Ck-qgeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        sources: list[str],\n",
        "        targets: list[str],\n",
        "        source_tokenizer: Tokenizer,\n",
        "        target_tokenizer: Tokenizer\n",
        "    ):\n",
        "        # Преобразование исходных и целевых последовательностей в индексы токенов\n",
        "        self.sources = [sequence.ids for sequence in source_tokenizer.encode_batch(sources)]\n",
        "        self.targets = [sequence.ids for sequence in target_tokenizer.encode_batch(targets)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sources)\n",
        "\n",
        "    def __getitem__(self, index: int) -> dict:\n",
        "        # Получение одного образца данных из набора данных по индексу\n",
        "        source = self.sources[index]\n",
        "        target = self.targets[index]\n",
        "        return {\n",
        "            \"input_sequence\": source,\n",
        "            \"target_sequence\": target\n",
        "        }\n",
        "\n",
        "def split_train_test(dataset: MyDataset, batch_size=32, validation_split=0.2) -> tuple[DataLoader, DataLoader]:\n",
        "    # Разделение набора данных на обучающую и валидационную выборки\n",
        "    total_samples = len(dataset)\n",
        "    val_size = int(validation_split * total_samples)\n",
        "    train_size = total_samples - val_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    # Создание загрузчиков данных для обучения и валидации\n",
        "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "    return train_data_loader, val_data_loader\n",
        "\n",
        "# Создание экземпляра набора данных\n",
        "dataset = MyDataset(sources, targets, source_tokenizer, target_tokenizer)\n",
        "\n",
        "# Разделение данных на обучающую и валидационную выборки с помощью функции split_train_test\n",
        "train_data_loader, val_data_loader = split_train_test(dataset)"
      ],
      "metadata": {
        "id": "FHdoS30Fqcqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Создание модели"
      ],
      "metadata": {
        "id": "KXmobnnftCun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Подготовка данных: важные переменные**\n",
        "\n",
        "- `sources` - список исходных текстов на русском\n",
        "- `targets` - список переводов текстов на лиасфирский\n",
        "- `source_tokenizer` - токенизатор для исходного языка (русский)\n",
        "- `target_tokenizer` - токенизатор для целевого языка перевода (лиасфирский)\n",
        "- `dataset` - набор данных, исходные и целевые, преобразованные в числовые идентификаторы\n",
        "- `train_data_loader` - загрузчики данных для основного обучения, тексты перемешаны и распределены по батчам (с моделью)\n",
        "- `val_data_loader` - для валидации (с моделью)"
      ],
      "metadata": {
        "id": "qSv86inRtNQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "kmhvJBsNrTyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, dropout=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedding = self.embedding(x)  # Преобразование входных токенов в векторные представления\n",
        "        outputs, hidden = self.rnn(embedding)  # Передача векторов через RNN\n",
        "        return outputs, hidden  # Возвращение выходов и последнего скрытого состояния"
      ],
      "metadata": {
        "id": "kjxG4UbQrOqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        timestep = encoder_outputs.shape[0]\n",
        "        hiddens = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n",
        "        encoder_outputs = encoder_outputs.transpose(0, 1)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hiddens, encoder_outputs), dim=2)))\n",
        "        attention = torch.softmax(torch.sum(self.v * energy, dim=2), dim=1).unsqueeze(1)\n",
        "        return attention"
      ],
      "metadata": {
        "id": "wP7hATIOr08g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout, attention):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.GRU(hidden_size + embedding_size, hidden_size, num_layers, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
        "        self.attention = attention\n",
        "\n",
        "    def forward(self, x, hidden, encoder_outputs):\n",
        "        x = x.unsqueeze(0)\n",
        "        embedding = self.embedding(x)  # Преобразование целевых токенов в векторные представления\n",
        "        attention = self.attention(hidden, encoder_outputs)  # Вычисление весов внимания\n",
        "        attention_weights = torch.bmm(attention, encoder_outputs.transpose(0, 1))\n",
        "        attention_weights = attention_weights.transpose(0, 1)\n",
        "        rnn_input = torch.cat((embedding, attention_weights), dim=2)  # Объединение входных токенов с контекстным вектором\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        output = output.squeeze(0)\n",
        "        attention_weights = attention_weights"
      ],
      "metadata": {
        "id": "Ev6JKXcPr5eE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "        batch_size = source.shape[1]\n",
        "        target_len = target.shape[0]\n",
        "        target_vocab_size = len(target_tokenizer.word2index)\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "        encoder_outputs, hidden = self.encoder(source)\n",
        "        x = target[0]\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden = self.decoder(x, hidden, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            best_guess = output.argmax(1)\n",
        "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "HmA0aHH8sJf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Параметры модели\n",
        "\n",
        "input_size = source_tokenizer.get_vocab_size()\n",
        "output_size = target_tokenizer.get_vocab_size()\n",
        "embedding_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 2\n",
        "dropout = 0.5\n",
        "learning_rate = 0.001\n",
        "pad_index = target_tokenizer.encode(\"[PAD]\", add_special_tokens=False)  # Индекс для паддинга\n",
        "start_index = target_tokenizer.encode(\"[SOS]\", add_special_tokens=False)  # Индекс начала последовательности\n",
        "end_index = target_tokenizer.encode(\"[EOS]\", add_special_tokens=False)  # Индекс конца последовательности"
      ],
      "metadata": {
        "id": "t-tsM37ctONh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "encoder = Encoder(input_size, embedding_size, hidden_size, num_layers, dropout).to(device)\n",
        "attention = Attention(hidden_size).to(device)\n",
        "decoder = Decoder(output_size, embedding_size, hidden_size, output_size, num_layers, dropout, attention).to(device)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jhY5pXQsUxY",
        "outputId": "8b562fed-66ef-46a7-c658-5a985f12ae8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(2000, 256)\n",
            "    (rnn): GRU(256, 512, num_layers=2, dropout=0.5)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embedding): Embedding(2000, 256)\n",
            "    (rnn): GRU(768, 512, num_layers=2, dropout=0.5)\n",
            "    (fc): Linear(in_features=1024, out_features=2000, bias=True)\n",
            "    (attention): Attention(\n",
            "      (attn): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    }
  ]
}